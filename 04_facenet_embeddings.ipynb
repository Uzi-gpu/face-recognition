{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# FaceNet Embeddings with MTCNN\n",
                "\n",
                "State-of-the-art face recognition using FaceNet architecture and MTCNN face detection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from mtcnn import MTCNN\n",
                "import cv2\n",
                "\n",
                "print('MTCNN imported successfully')\n",
                "print('MTCNN: Multi-task Cascaded Convolutional Networks')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MTCNN Face Detector\n",
                "\n",
                "MTCNN simultaneously detects:\n",
                "- **Faces** (bounding boxes)\n",
                "- **Facial landmarks** (eyes, nose, mouth corners)\n",
                "- **Confidence scores**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize MTCNN detector\n",
                "detector = MTCNN()\n",
                "print('✓ MTCNN detector initialized')\n",
                "\n",
                "# MTCNN specifications\n",
                "specs = {\n",
                "    'Detector': 'Multi-task CNN',\n",
                "    'Outputs': 'Boxes + Landmarks + Confidence',\n",
                "    'Landmarks': '5 points (eyes, nose, mouth)',\n",
                "    'Accuracy': '99%+ detection rate',\n",
                "    'Use case': 'Face alignment for recognition'\n",
                "}\n",
                "\n",
                "print('\\nMTCNN Specifications:')\n",
                "for key, value in specs.items():\n",
                "    print(f'  {key}: {value}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Demonstrate MTCNN Detection\n",
                "\n",
                "Create a sample detection to show MTCNN capabilities:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a demo image\n",
                "demo_img = np.ones((300, 400, 3), dtype=np.uint8) * 255\n",
                "\n",
                "# Simulated detection result\n",
                "detection = {\n",
                "    'box': [50, 50, 200, 200],\n",
                "    'confidence': 0.9987,\n",
                "    'keypoints': {\n",
                "        'left_eye': (100, 100),\n",
                "        'right_eye': (200, 100),\n",
                "        'nose': (150, 150),\n",
                "        'mouth_left': (120, 200),\n",
                "        'mouth_right': (180, 200)\n",
                "    }\n",
                "}\n",
                "\n",
                "# Draw bounding box\n",
                "x, y, w, h = detection['box']\n",
                "cv2.rectangle(demo_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
                "\n",
                "# Draw landmarks\n",
                "for name, (px, py) in detection['keypoints'].items():\n",
                "    cv2.circle(demo_img, (px, py), 5, (255, 0, 0), -1)\n",
                "\n",
                "# Add label\n",
                "label = f\"Confidence: {detection['confidence']:.2%}\"\n",
                "cv2.putText(demo_img, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.imshow(cv2.cvtColor(demo_img, cv2.COLOR_BGR2RGB))\n",
                "plt.title('MTCNN Detection Demo')\n",
                "plt.axis('off')\n",
                "plt.show()\n",
                "\n",
                "print('\\n✓ MTCNN detects face + 5 landmarks')\n",
                "print('✓ Landmarks used for face alignment')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## FaceNet Architecture\n",
                "\n",
                "FaceNet uses Inception ResNet v1:\n",
                "- **Input**: 160×160 RGB image\n",
                "- **Output**: 512-dimensional embedding\n",
                "- **Training**: Triplet loss\n",
                "- **Accuracy**: 99.65% on LFW"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# FaceNet architecture overview\n",
                "architecture = {\n",
                "    'Model': 'Inception ResNet v1',\n",
                "    'Parameters': '23.6 million',\n",
                "    'Input size': '160×160×3',\n",
                "    'Embedding size': '512 dimensions',\n",
                "    'Training dataset': 'VGGFace2 (3.31M images)',\n",
                "    'Loss function': 'Triplet loss',\n",
                "    'Accuracy (LFW)': '99.65%'\n",
                "}\n",
                "\n",
                "print('='*60)\n",
                "print('FACENET ARCHITECTURE')\n",
                "print('='*60)\n",
                "for key, value in architecture.items():\n",
                "    print(f'{key:20} : {value}')\n",
                "print('='*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Triplet Loss Explained\n",
                "\n",
                "FaceNet is trained using triplet loss:\n",
                "- **Anchor**: Reference face\n",
                "- **Positive**: Same person (different photo)\n",
                "- **Negative**: Different person\n",
                "\n",
                "**Goal**: Make ||anchor - positive|| small and ||anchor - negative|| large"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate triplet loss concept\n",
                "anchor = np.random.randn(512)\n",
                "positive = anchor + np.random.randn(512) * 0.1  # Same person, similar embedding\n",
                "negative = np.random.randn(512)  # Different person\n",
                "\n",
                "# Calculate distances\n",
                "dist_positive = np.linalg.norm(anchor - positive)\n",
                "dist_negative = np.linalg.norm(anchor - negative)\n",
                "\n",
                "print('Triplet Loss Example:')\n",
                "print(f'  Distance (anchor ↔ positive): {dist_positive:.4f}')\n",
                "print(f'  Distance (anchor ↔ negative): {dist_negative:.4f}')\n",
                "print()\n",
                "print(f'  Margin: {dist_negative - dist_positive:.4f}')\n",
                "print('  Goal: Maximize this margin!')\n",
                "print()\n",
                "print('✓ Triplet loss ensures same-person faces cluster together')\n",
                "print('✓ Different people\\'s faces stay far apart')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Embedding Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate FaceNet embeddings\n",
                "person_x_emb1 = np.random.randn(512)\n",
                "person_x_emb2 = person_x_emb1 + np.random.randn(512) * 0.15\n",
                "person_y_emb = np.random.randn(512)\n",
                "\n",
                "# Euclidean distance\n",
                "dist_same = np.linalg.norm(person_x_emb1 - person_x_emb2)\n",
                "dist_diff = np.linalg.norm(person_x_emb1 - person_y_emb)\n",
                "\n",
                "# Cosine similarity\n",
                "cos_same = np.dot(person_x_emb1, person_x_emb2) / (\n",
                "    np.linalg.norm(person_x_emb1) * np.linalg.norm(person_x_emb2)\n",
                ")\n",
                "cos_diff = np.dot(person_x_emb1, person_y_emb) / (\n",
                "    np.linalg.norm(person_x_emb1) * np.linalg.norm(person_y_emb)\n",
                ")\n",
                "\n",
                "print('\\n' + '='*60)\n",
                "print('EMBEDDING COMPARISON')\n",
                "print('='*60)\n",
                "print(f'Same Person (X vs X):')\n",
                "print(f'  Euclidean distance: {dist_same:.4f} (threshold: <0.7)')\n",
                "print(f'  Cosine similarity:  {cos_same:.4f} (threshold: >0.6)')\n",
                "print()\n",
                "print(f'Different People (X vs Y):')\n",
                "print(f'  Euclidean distance: {dist_diff:.4f}')\n",
                "print(f'  Cosine similarity:  {cos_diff:.4f}')\n",
                "print('='*60)\n",
                "print('\\n✓ Clear separation between same/different people')\n",
                "print('✓ FaceNet achieves 99.65% accuracy on LFW')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}